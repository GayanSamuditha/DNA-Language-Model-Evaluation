# DNA-Language-Model-Evaluation
Unsupervised evaluation of pre-trained DNA language model embeddings


DNA language models (DLMs) have generated significant excitement for their potential to revolutionize genetics tasks, including gene finding, enhancer annotation, and histone modification. Despite their successes, these models face challenges in capturing individual-level transcriptome variation, underscoring the need for more comprehensive evaluation methods. Existing evaluation approaches often rely on computationally intensive downstream tasks and may not adequately assess a model's generalist capabilities.
This repository presents an unsupervised evaluation framework for pre-trained DNA language model embeddings. It aims to address the limitations of current evaluation methods by providing robust and efficient metrics to gauge model performance and generalization. Our approach emphasizes the assessment of models beyond specific tasks, focusing on their ability to learn and generalize across a range of genetic contexts.
Explore the code and methodologies for evaluating DLMs, and contribute to advancing the field of DNA language modeling through improved evaluation techniques.
