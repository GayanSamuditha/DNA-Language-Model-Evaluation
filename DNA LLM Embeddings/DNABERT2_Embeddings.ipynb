{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f389523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install triton==2.0.0.dev20221103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf47fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install transformers==4.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98949504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from Bio import SeqIO\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be132b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rxa615/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/rxa615/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2a98af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rxa615'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a54a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/.../.../.../.../Unsupervised_Evaluation_LLM/PaperWork/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ee955b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = '/.../.../.../.../Unsupervised_Evaluation_LLM/PaperWork/datasets'\n",
    "csv_files = [file for file in os.listdir(datasets_folder) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ebf6885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_demo_coding_vs_intergenomic_seqs.csv',\n",
       " 'test_human_enhancers_cohn.csv',\n",
       " 'test_human_enhancers_ensembl.csv',\n",
       " 'test_human_ensembl_regulatory.csv',\n",
       " 'test_human_nontata_promoters.csv',\n",
       " 'test_human_ocr_ensembl.csv',\n",
       " 'train_demo_coding_vs_intergenomic_seqs.csv',\n",
       " 'train_human_enhancers_cohn.csv',\n",
       " 'train_human_enhancers_ensembl.csv',\n",
       " 'train_human_ensembl_regulatory.csv',\n",
       " 'train_human_nontata_promoters.csv',\n",
       " 'train_human_ocr_ensembl.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f38917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_human_enhancers_ensembl.csv', 'train_human_ensembl_regulatory.csv', 'train_human_nontata_promoters.csv', 'train_human_ocr_ensembl.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = csv_files[8:12]\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c52fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define a function to calculate mean embedding\n",
    "def calculate_mean_embedding(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt')[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        hidden_states = model(inputs)[0]  # [1, sequence_length, 768]\n",
    "\n",
    "    # Embedding with mean pooling\n",
    "    embedding_mean = torch.mean(hidden_states, dim=1).squeeze()  # Calculate mean along the sequence length\n",
    "    # Convert torch tensor to numpy array\n",
    "    embedding_mean_np = embedding_mean.detach().numpy()\n",
    "\n",
    "    return embedding_mean_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef6e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_human_enhancers_ensembl.csv\n",
      "(123872, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1847/123872 [01:22<1:42:02, 19.93it/s]/home/rxa615/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 512 to 520\n",
      "  warnings.warn(\n",
      " 48%|████▊     | 59186/123872 [44:10<43:53, 24.56it/s]   /home/rxa615/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT-2-117M/d064dece8a8b41d9fb8729fbe3435278786931f1/bert_layers.py:433: UserWarning: Increasing alibi size from 520 to 549\n",
      "  warnings.warn(\n",
      "100%|██████████| 123872/123872 [1:32:07<00:00, 22.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved successfully at: /.../.../.../.../Unsupervised_Evaluation_LLM/PaperWork/DNA_BERT2/train_human_enhancers_ensembl_DB2_Embeddings.npy\n",
      "train_human_ensembl_regulatory.csv\n",
      "(231348, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 32463/231348 [43:00<3:56:03, 14.04it/s]"
     ]
    }
   ],
   "source": [
    "# Folder path to save the embeddings and the time calculation file\n",
    "output_folder_path = \"/.../.../.../.../Unsupervised_Evaluation_LLM/PaperWork/DNA_BERT2\"\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# List to store the time taken for each file\n",
    "time_records = []\n",
    "\n",
    "# Apply the calculate_mean_embedding function to each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(datasets_folder, csv_file)\n",
    "    print(csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.shape)\n",
    "    # Measure the time taken to calculate the mean embeddings\n",
    "    start_time = time.time()\n",
    "    tqdm.pandas()\n",
    "    df['mean_embedding'] = df['seq'].progress_apply(calculate_mean_embedding)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Save the time taken for the current file\n",
    "    time_records.append({'file_name': csv_file, 'time_taken': time_taken})\n",
    "\n",
    "    # Save the 'mean_embedding' column as a numpy array\n",
    "    embedding_file_path = os.path.join(output_folder_path, os.path.splitext(csv_file)[0] + \"_DB2_Embeddings.npy\")\n",
    "    np.save(embedding_file_path, df['mean_embedding'].values)\n",
    "\n",
    "    print(f\"Embeddings saved successfully at: {embedding_file_path}\")\n",
    "\n",
    "# # Save the time calculation records to a CSV file\n",
    "# time_df = pd.DataFrame(time_records)\n",
    "# time_csv_path = os.path.join(output_folder_path, \"DB2_time_calculation.csv\")\n",
    "# time_df.to_csv(time_csv_path, index=False)\n",
    "\n",
    "# print(f\"Time calculations saved successfully at: {time_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0eeec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
