{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d259c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-nccl-cu12 (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-nccl-cu12 (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/job.1624582.hpc/pip-req-build-jxd5qqi0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/job.1624582.hpc/pip-req-build-jxd5qqi0\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 594c1610fa6243b2ffb670c49faf389ca5121939\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (23.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/easybuild_allnodes/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (2022.4.24)\n",
      "Requirement already satisfied: requests in /usr/local/easybuild_allnodes/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.local/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/easybuild_allnodes/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (1.22.3)\n",
      "Requirement already satisfied: filelock in /usr/local/easybuild_allnodes/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.10/site-packages (from transformers==4.43.0.dev0) (0.23.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.0.dev0) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/easybuild_allnodes/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from requests->transformers==4.43.0.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests->transformers==4.43.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/easybuild_allnodes/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from requests->transformers==4.43.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/easybuild_allnodes/software/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/site-packages (from requests->transformers==4.43.0.dev0) (2021.10.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-nccl-cu12 (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-nccl-cu12 (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-nccl-cu12 (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-nccl-cu12 (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/rxa615/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/easybuild_allnodes/software/Python/3.10.4-GCCcore-11.3.0/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf5c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers -U "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f23f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "from Bio import SeqIO\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0828b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rxa615/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Import the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612b74d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rxa615'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c78b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/mnt/vstor/SOM_EPBI_XXZ10/raghav/Unsupervised_Evaluation_LLM/PaperWork/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7eecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder = '/mnt/vstor/SOM_EPBI_XXZ10/raghav/Unsupervised_Evaluation_LLM/PaperWork/datasets'\n",
    "csv_files = [file for file in os.listdir(datasets_folder) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560c2047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_demo_coding_vs_intergenomic_seqs.csv',\n",
       " 'test_human_enhancers_cohn.csv',\n",
       " 'test_human_enhancers_ensembl.csv',\n",
       " 'test_human_ensembl_regulatory.csv',\n",
       " 'test_human_nontata_promoters.csv',\n",
       " 'test_human_ocr_ensembl.csv',\n",
       " 'train_demo_coding_vs_intergenomic_seqs.csv',\n",
       " 'train_human_enhancers_cohn.csv',\n",
       " 'train_human_enhancers_ensembl.csv',\n",
       " 'train_human_ensembl_regulatory.csv',\n",
       " 'train_human_nontata_promoters.csv',\n",
       " 'train_human_ocr_ensembl.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6255dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_demo_coding_vs_intergenomic_seqs.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files = csv_files[6:7]\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbc76a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define a function to calculate mean embedding for a list of sequences\n",
    "def calculate_mean_embeddings(sequences, tokenizer, model):\n",
    "    all_mean_sequence_embeddings = []\n",
    "    \n",
    "    # Set max length based on the tokenizer's model maximum length\n",
    "    max_length = tokenizer.model_max_length\n",
    "    \n",
    "    # Get the number of sequences\n",
    "    num_sequences = len(sequences)\n",
    "    \n",
    "    # Iterate through sequences in batches\n",
    "    for i in tqdm(range(0, num_sequences, batch_size)):\n",
    "        # Get sequences for the current batch\n",
    "        batch_sequences = sequences[i:i+batch_size]\n",
    "        # Tokenize sequences\n",
    "        tokens_ids = tokenizer.batch_encode_plus(batch_sequences, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "       \n",
    "        # Compute the embeddings\n",
    "        attention_mask = tokens_ids != tokenizer.pad_token_id\n",
    "        torch_outs = model(\n",
    "            tokens_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Compute sequences embeddings\n",
    "        embeddings = torch_outs['hidden_states'][-1].detach().numpy()\n",
    "        \n",
    "        # Add embed dimension axis\n",
    "        attention_mask = np.expand_dims(attention_mask.numpy(), axis=-1)\n",
    "\n",
    "        # Compute mean embeddings per sequence\n",
    "        mean_sequence_embeddings = np.sum(attention_mask * embeddings, axis=-2) / np.sum(attention_mask, axis=1)\n",
    "        \n",
    "        # Append to the list\n",
    "        all_mean_sequence_embeddings.append(mean_sequence_embeddings)\n",
    "    \n",
    "    # Concatenate all the mean embeddings\n",
    "    all_mean_sequence_embeddings = np.concatenate(all_mean_sequence_embeddings, axis=0)\n",
    "    \n",
    "    return all_mean_sequence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c1bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_demo_coding_vs_intergenomic_seqs.csv\n",
      "(75000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/7500 [07:54<42:27:35, 20.44s/it]"
     ]
    }
   ],
   "source": [
    "output_folder_path = \"/mnt/vstor/SOM_EPBI_XXZ10/raghav/Unsupervised_Evaluation_LLM/PaperWork/nucleotide-transformer-500m-human-ref/\"\n",
    "\n",
    "\n",
    "# Initialize the list to store all mean sequence embeddings\n",
    "all_mean_sequence_embeddings = []\n",
    "\n",
    "# Apply the calculate_mean_embeddings function to each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(datasets_folder, csv_file)\n",
    "    print(csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.shape)\n",
    "\n",
    "    tqdm.pandas()\n",
    "    sequences = df['seq'].tolist()\n",
    "    batch_size = 10\n",
    "    mean_embeddings = calculate_mean_embeddings(sequences, tokenizer, model)\n",
    "\n",
    "    # Save the 'mean_embedding' column as a numpy array\n",
    "    embedding_file_path = os.path.join(output_folder_path, os.path.splitext(csv_file)[0] + \"_NT_Embeddings.npy\")\n",
    "    np.save(embedding_file_path, mean_embeddings)\n",
    "    print(f\"Embeddings saved successfully at: {embedding_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7d162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
